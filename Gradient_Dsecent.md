

# Gradient Descent
## 方式
1）SGD  
2）mini-batch GD
3)Batch GD

## 学习率的优化 
### Adagrad  
<img src="https://github.com/liuzhenfang001/ML/blob/main/fig/Adagrad1.png" width="600" height="400" alt="Adagrad"/><br/>  
1)分开考虑每一个参数，因为这些参数的学习率不同
<img src="https://user-images.githubusercontent.com/43060711/125901627-22aa6b5f-e9eb-47ba-9789-752ca6a8daef.png" width="600" height="400" alt="Adagrad"/><br/>  

