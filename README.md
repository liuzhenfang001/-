# 秋招准备  
## 机器学习资源
机器学习模型可以分为两派：频率派和贝叶斯派  
频率派：逻辑回归，线性回归，SVM， Adaboost， XGBoost， LightGBM， KNN， K-means， Bagging, Boosting, 朴素贝叶斯，随机森林，GMM， 最大熵模型， PCA， LDA，SVD， 决策树  
贝叶斯派：EM， 马尔科夫随机场， 条件随机场， 隐马尔可夫模型（HMM）  

### 1. 逻辑回归：
https://zhuanlan.zhihu.com/p/74874291
  #### 为什么要加上一层SIGMOD函数？  
   答：线性模型（也可以是非线性模型，例如二次）的输出是连续的，不能很好的拟合分类（离散）问题。假设二分类，线性模型输出正，则判断为正例，负数则判断为负例，那么无法进行优化。加上SIGMOD，优化会尽可能的让模型输出正无穷或负无穷的值，例如使用MLE进行参数估计。另外，逻辑回归能预测出概率值，可以辅助决策。  
   #### 优化
   随机梯度下降SGD：迭代次数用完或者损失小于某个阈值则停止  
   牛顿法： 思想是在现有的极小点附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值  
   #### 正则化  
   Lasso回归：相当于在似然函数后加上L1正则化项，本质是为模型增加“模型参数w服从零均值拉普拉斯分布”这一先验知识。得到的参数比较稀疏（个别参数几乎为0）  
   Ridge回归：相当于为模型添加这样的先验知识：w服从零均值正态分布。得到的参数比较平滑（接近于0）  
   链接里面关于L1和L2正则的原理和区别讲的很清楚  
   
   #### LR与线性回归的区别
   一个分类，一个回归，两者都属于广义线性模型。  
   SIGMOD的作用：  
   1）线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；  
   2）线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。  
   #### 与最大熵的区别  
   本质相同
   #### 与SVM
   相同点：  
   1）都是分类算法，本质上都是在找最佳分类超平面；  
   2）都是监督学习算法；  
   3）都是判别式模型，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；  
   4）都可以增加不同的正则项。  
   不同点：  
   1）LR 是一个统计的方法，SVM 是一个几何的方法；  
   2）SVM 的处理方法是只考虑 Support Vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；  
   3）损失函数不同：LR 的损失函数是交叉熵，SVM 的损失函数是 HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对 HingeLoss 来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；  
   4）LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；  
   5）LR 可以产生概率，SVM 不能；  
   6）LR 不依赖样本之间的距离，SVM 是基于距离的；  
   7）LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算  
  #### 与朴素贝叶斯  
  #### 为什么LR适合离散特征
  #### 为什么不用平方误差？
### 2. 线性回归
### 3. 朴素贝叶斯（生成式模型）
https://zhuanlan.zhihu.com/p/26262151  
#### 较强的假设
  假设特征之间是相互独立的。  
 #### 为什么假设是独立？
  根据贝叶斯公式，要计算后验概率P(y|x)，需要求出条件概率P(x|y)，先验P(x),P(y)，然而x的特征x1,x2...xn的随意组合在样本空间是稀疏的，如果假设特征独立，就有：  
  P(x|y)=P(x1,x2,...,xn|y)=P(x1|y) * P(x2|y) *...* P(xn|y)  
  P(x)=P(x1)P(x2)...P(xn)  
 #### 优缺点  
 优点：  
 1）算法逻辑简单  
 2）分类过程时空开销小  
 缺点：  
理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。

而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
### 4.决策树  



