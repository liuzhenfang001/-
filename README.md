# 秋招准备  
## 机器学习分类
机器学习模型可以分为两派：频率派和贝叶斯派  
频率派：逻辑回归，线性回归，SVM， Adaboost， XGBoost， LightGBM， KNN， K-means， Bagging, Boosting, 朴素贝叶斯，随机森林，GMM， 最大熵模型， PCA， LDA，SVD， 决策树  
贝叶斯派：EM， 马尔科夫随机场， 条件随机场， 隐马尔可夫模型（HMM）  
  
## 生成模型 与 判别模型：  
生成模型  
高斯混合、隐马尔可夫，朴素贝叶斯  
判别模型：  
除了生成模型   
区别：生成模型是求联合概率，首先学习出不同类别样本的分布模型，对于新来的测试样本x，求P(x,y)最大的一类，即x的最终分类。注意，联合概率的和不为1。  
判别模型免去了学习模型的过程，而是直接找到最优的判定界面，根据判定界面来对测试样本x分类，也就是后验概率P(y|x),一般来说，后验概率和为1。
## 1. 逻辑回归：
https://zhuanlan.zhihu.com/p/74874291
  #### 为什么要加上一层SIGMOD函数？  
   答：线性模型（也可以是非线性模型，例如二次）的输出是连续的，不能很好的拟合分类（离散）问题。假设二分类，线性模型输出正，则判断为正例，负数则判断为负例，那么无法进行优化。加上SIGMOD，优化会尽可能的让模型输出正无穷或负无穷的值，例如使用MLE进行参数估计。另外，逻辑回归能预测出概率值，可以辅助决策。  
   #### 优化
   随机梯度下降SGD：迭代次数用完或者损失小于某个阈值则停止  
   牛顿法： 思想是在现有的极小点附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值  
   #### 正则化  
   Lasso回归：相当于在似然函数后加上L1正则化项，本质是为模型增加“模型参数w服从零均值拉普拉斯分布”这一先验知识。得到的参数比较稀疏（个别参数几乎为0）  
   Ridge回归：相当于为模型添加这样的先验知识：w服从零均值正态分布。得到的参数比较平滑（接近于0）  
   链接里面关于L1和L2正则的原理和区别讲的很清楚  
   
   #### 多分类问题softmax | OVSO | OVSR
   https://zhuanlan.zhihu.com/p/62381502  
   
   #### LR与线性回归的区别
   一个分类，一个回归，两者都属于广义线性模型。  
   SIGMOD的作用：  
   1）线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；  
   2）线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。  
   #### 与最大熵的区别  
   本质相同
   #### 与SVM
   相同点：  
   1）都是分类算法，本质上都是在找最佳分类超平面；  
   2）都是监督学习算法；  
   3）都是判别式模型，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；  
   4）都可以增加不同的正则项。  
   不同点：  
   1）LR 是一个统计的方法，SVM 是一个几何的方法；  
   2）SVM 的处理方法是只考虑 Support Vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；  
   3）损失函数不同：LR 的损失函数是交叉熵，SVM 的损失函数是 HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对 HingeLoss 来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；  
   4）LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；  
   5）LR 可以产生概率，SVM 不能；  
   6）LR 不依赖样本之间的距离，SVM 是基于距离的；  
   7）LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算  
  #### 与朴素贝叶斯  
  #### 为什么LR适合离散特征
  #### 为什么不用平方误差？
## 2. 线性回归
## 3. 朴素贝叶斯（生成式模型）
https://zhuanlan.zhihu.com/p/26262151  
#### 较强的假设
  假设特征之间是相互独立的。  
 #### 为什么假设是独立？
  根据贝叶斯公式，要计算后验概率P(y|x)，需要求出条件概率P(x|y)，先验P(x),P(y)，然而x的特征x1,x2...xn的随意组合在样本空间是稀疏的，如果假设特征独立，就有：  
  P(x|y)=P(x1,x2,...,xn|y)=P(x1|y) * P(x2|y) *...* P(xn|y)  
  P(x)=P(x1)P(x2)...P(xn)  
 #### 优缺点  
 优点：  
 1）算法逻辑简单  
 2）分类过程时空开销小  
 缺点：  
理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。

而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
## 4.决策树  
## 5.GMM  
  https://zhuanlan.zhihu.com/p/30483076
  #### GMM无法使用MLE解出来：
  因为含有隐变量ak，即x属于第k个高斯的概率,log似然里面含有求和项，无法计算梯度。  
  #### GMM参数估计  
  EM算法：
  1.随机初始化参数  
  2.E：求期望  
  3.M:求极大，计算新一轮的参数  
  终止条件：重复计算 E-step 和 M-step 直至收敛 ，即前后参数变化小于某个阈值  
    
   需要注意的是，EM 算法具备收敛性，但并不保证找到全局最大值，有可能找到局部最大值。解决方法是初始化几次不同的参数进行迭代，取结果最好的那次。

## 集成学习
  构建并结合多个学习器。每个学习器得到的结果，最终以少数服从多数，决定分类结果。
### -Boosting
Boosting是优化残差的过程，下一个学习器拟合上一个学习器的残差，学习器之间具有依赖关系。准确率高，但是速度慢。  

#### Adaboost
#### 提升树
1）加法模型

#### GBDT（梯度提升树）

#### XGBoost， 
### -Bagging 
学习器相互独立，可以并发学习，速度快，但是准确度不高。  
Bagging就是随机取数据：又放回的随机取m条数据，剩余的作为测试集，从而训练得到学习器。  
可以解决单学习器产生的过拟合：可以理解为大数定律，正态分布等。
#### 随机森林
  随机森林，不仅要随机取数据，还要随机的取属性（特征），随机森林基础是决策树  
  如何特征选择？  
  为特征添加噪声，如果加了噪声比不加噪声得到的结果差很多，那么说明这种特征比较重要，根据重要性进行排名。  
  1）然后选取一个比率（例如20%），删除影响力（重要性）最小的20%的特征，
  2）重复计算排名  
  3）循环的删除特征，直到达到期望的特征数。  
  
